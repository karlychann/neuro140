{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karlychann/neuro140/blob/main/uvfp_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b6fffff-5577-40e2-9bd7-b6f88dcc6b7d",
      "metadata": {
        "id": "2b6fffff-5577-40e2-9bd7-b6f88dcc6b7d"
      },
      "source": [
        "# Final models trained on the entire dataset, with and without biased features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ffe31e54-576e-4c25-ab9d-33b7a76adc98",
      "metadata": {
        "id": "ffe31e54-576e-4c25-ab9d-33b7a76adc98"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from io import BytesIO\n",
        "import pickle\n",
        "import requests\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "bdab5dfe-b099-4cd2-92b9-1a22ff9f555f",
      "metadata": {
        "id": "bdab5dfe-b099-4cd2-92b9-1a22ff9f555f"
      },
      "outputs": [],
      "source": [
        "all_features = ['F0semitoneFrom27.5Hz_sma3nz_amean',\n",
        "       'F0semitoneFrom27.5Hz_sma3nz_stddevNorm',\n",
        "       'F0semitoneFrom27.5Hz_sma3nz_percentile20.0',\n",
        "       'F0semitoneFrom27.5Hz_sma3nz_percentile50.0',\n",
        "       'F0semitoneFrom27.5Hz_sma3nz_percentile80.0',\n",
        "       'F0semitoneFrom27.5Hz_sma3nz_pctlrange0-2',\n",
        "       'F0semitoneFrom27.5Hz_sma3nz_meanRisingSlope',\n",
        "       'F0semitoneFrom27.5Hz_sma3nz_stddevRisingSlope',\n",
        "       'F0semitoneFrom27.5Hz_sma3nz_meanFallingSlope',\n",
        "       'F0semitoneFrom27.5Hz_sma3nz_stddevFallingSlope', 'loudness_sma3_amean',\n",
        "       'loudness_sma3_stddevNorm', 'loudness_sma3_percentile20.0',\n",
        "       'loudness_sma3_percentile50.0', 'loudness_sma3_percentile80.0',\n",
        "       'loudness_sma3_pctlrange0-2', 'loudness_sma3_meanRisingSlope',\n",
        "       'loudness_sma3_stddevRisingSlope', 'loudness_sma3_meanFallingSlope',\n",
        "       'loudness_sma3_stddevFallingSlope', 'spectralFlux_sma3_amean',\n",
        "       'spectralFlux_sma3_stddevNorm', 'mfcc1_sma3_amean',\n",
        "       'mfcc1_sma3_stddevNorm', 'mfcc2_sma3_amean', 'mfcc2_sma3_stddevNorm',\n",
        "       'mfcc3_sma3_amean', 'mfcc3_sma3_stddevNorm', 'mfcc4_sma3_amean',\n",
        "       'mfcc4_sma3_stddevNorm', 'jitterLocal_sma3nz_amean',\n",
        "       'jitterLocal_sma3nz_stddevNorm', 'shimmerLocaldB_sma3nz_amean',\n",
        "       'shimmerLocaldB_sma3nz_stddevNorm', 'HNRdBACF_sma3nz_amean',\n",
        "       'HNRdBACF_sma3nz_stddevNorm', 'logRelF0-H1-H2_sma3nz_amean',\n",
        "       'logRelF0-H1-H2_sma3nz_stddevNorm', 'logRelF0-H1-A3_sma3nz_amean',\n",
        "       'logRelF0-H1-A3_sma3nz_stddevNorm', 'F1frequency_sma3nz_amean',\n",
        "       'F1frequency_sma3nz_stddevNorm', 'F1bandwidth_sma3nz_amean',\n",
        "       'F1bandwidth_sma3nz_stddevNorm', 'F1amplitudeLogRelF0_sma3nz_amean',\n",
        "       'F1amplitudeLogRelF0_sma3nz_stddevNorm', 'F2frequency_sma3nz_amean',\n",
        "       'F2frequency_sma3nz_stddevNorm', 'F2bandwidth_sma3nz_amean',\n",
        "       'F2bandwidth_sma3nz_stddevNorm', 'F2amplitudeLogRelF0_sma3nz_amean',\n",
        "       'F2amplitudeLogRelF0_sma3nz_stddevNorm', 'F3frequency_sma3nz_amean',\n",
        "       'F3frequency_sma3nz_stddevNorm', 'F3bandwidth_sma3nz_amean',\n",
        "       'F3bandwidth_sma3nz_stddevNorm', 'F3amplitudeLogRelF0_sma3nz_amean',\n",
        "       'F3amplitudeLogRelF0_sma3nz_stddevNorm', 'alphaRatioV_sma3nz_amean',\n",
        "       'alphaRatioV_sma3nz_stddevNorm', 'hammarbergIndexV_sma3nz_amean',\n",
        "       'hammarbergIndexV_sma3nz_stddevNorm', 'slopeV0-500_sma3nz_amean',\n",
        "       'slopeV0-500_sma3nz_stddevNorm', 'slopeV500-1500_sma3nz_amean',\n",
        "       'slopeV500-1500_sma3nz_stddevNorm', 'spectralFluxV_sma3nz_amean',\n",
        "       'spectralFluxV_sma3nz_stddevNorm', 'mfcc1V_sma3nz_amean',\n",
        "       'mfcc1V_sma3nz_stddevNorm', 'mfcc2V_sma3nz_amean',\n",
        "       'mfcc2V_sma3nz_stddevNorm', 'mfcc3V_sma3nz_amean',\n",
        "       'mfcc3V_sma3nz_stddevNorm', 'mfcc4V_sma3nz_amean',\n",
        "       'mfcc4V_sma3nz_stddevNorm', 'alphaRatioUV_sma3nz_amean',\n",
        "       'hammarbergIndexUV_sma3nz_amean', 'slopeUV0-500_sma3nz_amean',\n",
        "       'slopeUV500-1500_sma3nz_amean', 'spectralFluxUV_sma3nz_amean',\n",
        "       'loudnessPeaksPerSec', 'VoicedSegmentsPerSec',\n",
        "       'MeanVoicedSegmentLengthSec', 'StddevVoicedSegmentLengthSec',\n",
        "       'MeanUnvoicedSegmentLength', 'StddevUnvoicedSegmentLength',\n",
        "       'equivalentSoundLevel_dBp']\n",
        "\n",
        "\n",
        "# from classification_wo_correlated_features.ipynb features that correlate least with biased features\n",
        "uncorrelated_features = ['mfcc4V_sma3nz_amean',\n",
        "       'F0semitoneFrom27.5Hz_sma3nz_meanFallingSlope', 'mfcc1_sma3_amean',\n",
        "       'F3bandwidth_sma3nz_stddevNorm',\n",
        "       'F0semitoneFrom27.5Hz_sma3nz_stddevRisingSlope',\n",
        "       'F1frequency_sma3nz_stddevNorm', 'jitterLocal_sma3nz_stddevNorm',\n",
        "       'F0semitoneFrom27.5Hz_sma3nz_stddevFallingSlope',\n",
        "       'alphaRatioV_sma3nz_stddevNorm', 'mfcc1_sma3_stddevNorm',\n",
        "       'F0semitoneFrom27.5Hz_sma3nz_meanRisingSlope', 'mfcc4_sma3_amean',\n",
        "       'F3frequency_sma3nz_amean', 'mfcc2_sma3_amean',\n",
        "       'VoicedSegmentsPerSec', 'F1bandwidth_sma3nz_amean',\n",
        "       'mfcc2V_sma3nz_amean', 'F3frequency_sma3nz_stddevNorm',\n",
        "       'hammarbergIndexV_sma3nz_stddevNorm',\n",
        "       'logRelF0-H1-H2_sma3nz_amean', 'slopeV500-1500_sma3nz_stddevNorm',\n",
        "       'F2bandwidth_sma3nz_amean', 'mfcc3_sma3_amean',\n",
        "       'F2bandwidth_sma3nz_stddevNorm', 'alphaRatioV_sma3nz_amean',\n",
        "       'mfcc2_sma3_stddevNorm', 'mfcc1V_sma3nz_amean',\n",
        "       'slopeUV0-500_sma3nz_amean', 'mfcc1V_sma3nz_stddevNorm',\n",
        "       'mfcc3V_sma3nz_amean', 'F2frequency_sma3nz_amean',\n",
        "       'logRelF0-H1-A3_sma3nz_amean', 'hammarbergIndexV_sma3nz_amean',\n",
        "       'F1bandwidth_sma3nz_stddevNorm', 'mfcc3_sma3_stddevNorm',\n",
        "       'mfcc2V_sma3nz_stddevNorm', 'F1frequency_sma3nz_amean',\n",
        "       'F2frequency_sma3nz_stddevNorm',\n",
        "       'logRelF0-H1-H2_sma3nz_stddevNorm', 'mfcc4V_sma3nz_stddevNorm',\n",
        "       'mfcc4_sma3_stddevNorm', 'F3bandwidth_sma3nz_amean',\n",
        "       'mfcc3V_sma3nz_stddevNorm', 'slopeV0-500_sma3nz_stddevNorm']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6bd09080-1cb7-4a02-a352-8b95ee142994",
      "metadata": {
        "id": "6bd09080-1cb7-4a02-a352-8b95ee142994"
      },
      "outputs": [],
      "source": [
        "# load pretrained model\n",
        "# model_name = 'less-biased'\n",
        "# training_model_name = 'rf'\n",
        "# task = 'speech'\n",
        "# feature_set = uncorrelated_features\n",
        "\n",
        "# url_path = f'https://github.com/danielmlow/vfp/blob/main/data/output/{training_model_name}_{model_name}_{task}.pkl?raw=true' # speech models trained on reading task\n",
        "# mfile = BytesIO(requests.get(url_path).content) # load from url\n",
        "# model = pickle.load(mfile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "597c3ec7-b843-4b5e-be2f-34da98f6886e",
      "metadata": {
        "id": "597c3ec7-b843-4b5e-be2f-34da98f6886e",
        "outputId": "c24cc06c-73e9-4441-afe9-29598207e49e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(453, 44) (453,) (453, 96)\n"
          ]
        }
      ],
      "source": [
        "# let's test on our own data\n",
        "task = 'speech'\n",
        "feature_set = uncorrelated_features\n",
        "\n",
        "df = pd.read_csv(f'https://github.com/danielmlow/vfp/raw/main/data/input/features/egemaps_vector_{task}_cpp.csv', index_col = 0)\n",
        "\n",
        "# X is just getting the values of all the columns that is used in the model for each recording\n",
        "# Y is just getting the target value of all the recordings\n",
        "# numpy array is a low level array\n",
        "# pandas dataframe is a higher level abstraction to wrap around numpy array\n",
        "X = df[feature_set].values\n",
        "y = df['target'].values\n",
        "\n",
        "# test\n",
        "# y_pred = model.predict(X)\n",
        "# performance = roc_auc_score(y, y_pred) # Should get perfect performance, since it is testing on the training set\n",
        "# performance\n",
        "print(X.shape, y.shape, df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2c52bca-dde8-497d-9c06-ce3af579da85",
      "metadata": {
        "id": "c2c52bca-dde8-497d-9c06-ce3af579da85"
      },
      "source": [
        "# How to extract features on your own wav files using egemaps\n",
        "\n",
        "To test on your own data, the test set should match our features (egemaps) using the the same variables and sampling rate (16k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a32bd722-ac4c-43c2-a9c4-feaee136c125",
      "metadata": {
        "id": "a32bd722-ac4c-43c2-a9c4-feaee136c125"
      },
      "outputs": [],
      "source": [
        "# from os.path import exists\n",
        "# # config: depends whether you're on Google Colab or local\n",
        "\n",
        "\n",
        "# # Get URL from github csv by clicking on Download > Copy Link Address\n",
        "\n",
        "# load_from_google_drive = False\n",
        "\n",
        "# if load_from_google_drive:\n",
        "#       # On google colab\n",
        "#       # Mount GDrive and attach it to the colab for data I/O\n",
        "#     from google.colab import drive\n",
        "#     drive.mount('/content/drive')\n",
        "#     input_dir = '/content/drive/My Drive/datum/vfp/data/input/'\n",
        "#     output_dir = '/content/drive/My Drive/datum/vfp/data/output/'\n",
        "#     os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# else:\n",
        "#   # If using jupyter-lab or jupyter notebook, load locally:\n",
        "#   input_dir = './data/input/'\n",
        "#   output_dir = './data/output/'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "input_dir = '/content/drive/My Drive/neuro140/vfp/data/input/'\n",
        "output_dir = '/content/drive/My Drive/neuro140/vfp/data/output/'\n",
        "os.makedirs(output_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "jJmYwuj1Ekr5",
        "outputId": "d7d227d9-0434-465d-d808-8cc2bb857e45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "jJmYwuj1Ekr5",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/neuro140/vfp/data/archive.zip -d /content/drive/MyDrive/neuro140/vfp/data"
      ],
      "metadata": {
        "id": "Wld07JB9MhuR"
      },
      "id": "Wld07JB9MhuR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! grep Diagnosis: /content/drive/MyDrive/neuro140/vfp/data/VOICEDDATASET/.txt"
      ],
      "metadata": {
        "id": "9BXTHCCWPWGF"
      },
      "id": "9BXTHCCWPWGF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wfdb scipy"
      ],
      "metadata": {
        "id": "OFd3Ye6LNTUi"
      },
      "id": "OFd3Ye6LNTUi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wfdb\n",
        "from scipy.io.wavfile import write\n",
        "\n",
        "def wfdb_to_wav(input_path, output_path, channel=0):\n",
        "    \"\"\"\n",
        "    Convert a WFDB file to a WAV file.\n",
        "\n",
        "    Parameters:\n",
        "    - input_path: Path to the input WFDB file.\n",
        "    - output_path: Path to the output WAV file.\n",
        "    - channel: The channel of the WFDB file to convert (default is 0)\n",
        "    \"\"\"\n",
        "    # Read the WFDB file\n",
        "    record = wfdb.rdrecord(input_path)\n",
        "\n",
        "    # Extract the signal from the specified channel\n",
        "    signal = record.p_signal[:, channel]\n",
        "\n",
        "    # Normalize the signal to be in the range of int16 (required for WAV files)\n",
        "    signal_normalized = ((signal - signal.min()) / (signal.max() - signal.min()) * (2**15 - 1) - 2**15).astype('int16')\n",
        "\n",
        "    # Write the normalized signal to a WAV file\n",
        "    # Note: The sample rate is set according to the WFDB record's sampling frequency\n",
        "    write(output_path, record.fs, signal_normalized)\n",
        "\n",
        "# Example usage\n",
        "input_path = '/content/drive/MyDrive/neuro140/vfp/data/VOICEDDATASET/voice001'  # Change this to the path of your WFDB file\n",
        "output_path = '/content/drive/MyDrive/neuro140/vfp/data/VOICEDDATASET/voice001.wav'  # Change this to your desired output path\n",
        "wfdb_to_wav(input_path, output_path)\n"
      ],
      "metadata": {
        "id": "m6PXTqMpNNx5"
      },
      "id": "m6PXTqMpNNx5",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a161eda-f652-4fa1-a77c-e895c48dfd74",
      "metadata": {
        "id": "8a161eda-f652-4fa1-a77c-e895c48dfd74"
      },
      "outputs": [],
      "source": [
        "# !pip install -q opensmile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fec9f65c-9a42-4494-bf00-75165e06273e",
      "metadata": {
        "id": "fec9f65c-9a42-4494-bf00-75165e06273e"
      },
      "outputs": [],
      "source": [
        "# import glob\n",
        "# import opensmile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c357706f-f776-467a-b665-ecf4b068e78b",
      "metadata": {
        "id": "c357706f-f776-467a-b665-ecf4b068e78b"
      },
      "outputs": [],
      "source": [
        "# wav_dir = './../../../data/blake_private/audio/blake_16khz/*'\n",
        "# wav_paths = glob.glob(wav_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83f82a7b-60b3-4f6a-a093-e38b0145f4a6",
      "metadata": {
        "id": "83f82a7b-60b3-4f6a-a093-e38b0145f4a6"
      },
      "outputs": [],
      "source": [
        "# smile = opensmile.Smile(\n",
        "#             feature_set=opensmile.FeatureSet.eGeMAPSv02, #or path to conf: 'gemaps/eGeMAPSv02.conf'\n",
        "#             feature_level=feature_level,\n",
        "#             sampling_rate=16000,\n",
        "#             resample=True,\n",
        "#             # num_workers = 4,\n",
        "#             verbose=True,\n",
        "#         )\n",
        "# feature_vectors = smile.process_files([\n",
        "#     wav_path\n",
        "\n",
        "\n",
        "# ])\n",
        "# df = feature_vectors.reset_index()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25d3de98-07dd-40dc-b643-7d7e54e704db",
      "metadata": {
        "id": "25d3de98-07dd-40dc-b643-7d7e54e704db"
      },
      "source": [
        "# How we trained these models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "eec295ce-d4b7-48e5-939e-0cd3a9e9e14b",
      "metadata": {
        "id": "eec295ce-d4b7-48e5-939e-0cd3a9e9e14b"
      },
      "outputs": [],
      "source": [
        "# We choose Random Forest as it tends to have highest median classification across analyses\n",
        "model = RandomForestClassifier(n_estimators= 100)\n",
        "training_model_name = 'rf'\n",
        "\n",
        "# Others:\n",
        "# LogisticRegressionCV(solver='liblinear', penalty = 'l1', max_iter = 100)\n",
        "# MLPClassifier(alpha = 1, max_iter= 1000)\n",
        "# SGDClassifier(loss='log', penalty=\"elasticnet\", early_stopping=True, max_iter = 5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "a91cb19c-b4b6-4b6b-8a6f-6b5b35d2ad8c",
      "metadata": {
        "id": "a91cb19c-b4b6-4b6b-8a6f-6b5b35d2ad8c",
        "outputId": "890e1a69-c726-4226-aca6-2863d12baaaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(453, 88) (453,)\n",
            "performance: speech biased 1.0\n",
            "(453, 44) (453,)\n",
            "performance: speech less-biased 1.0\n",
            "(455, 88) (455,)\n",
            "performance: vowel biased 1.0\n",
            "(455, 44) (455,)\n",
            "performance: vowel less-biased 1.0\n"
          ]
        }
      ],
      "source": [
        "for task in ['speech', 'vowel']:\n",
        "    df = pd.read_csv(f'https://github.com/danielmlow/vfp/raw/main/data/input/features/egemaps_vector_{task}_cpp.csv', index_col = 0)\n",
        "\n",
        "    for feature_set, model_name in zip([all_features, uncorrelated_features],\n",
        "                                       ['biased', 'less-biased']\n",
        "                                      ):\n",
        "\n",
        "        X = df[feature_set].values\n",
        "        y = df['target'].values\n",
        "        print(X.shape, y.shape)\n",
        "\n",
        "        # train and save model\n",
        "\n",
        "        output_path = output_dir+f'{training_model_name}_{model_name}_{task}.pkl'\n",
        "\n",
        "        model = RandomForestClassifier(n_estimators= 100)\n",
        "        model.fit(X,y) # train\n",
        "\n",
        "        # save\n",
        "        # file that is opened is called f\n",
        "        with open(output_path,'wb') as f:\n",
        "            pickle.dump(model,f)\n",
        "\n",
        "        # load\n",
        "        # with open(output_path, 'rb') as f:\n",
        "        #     model = pickle.load(f)\n",
        "\n",
        "        # model is predicting on the training set, reporting the training accuracy\n",
        "        y_pred = model.predict(X)\n",
        "        performance = roc_auc_score(y, y_pred) # Should get perfect performance, since it is testing on the training set\n",
        "        print('performance:', task, model_name, performance)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93eb3ac4-292c-44bd-b78f-825e82fbce7a",
      "metadata": {
        "id": "93eb3ac4-292c-44bd-b78f-825e82fbce7a"
      },
      "source": [
        "# Other features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27c4abe5-1080-4046-b878-d12c67317ee6",
      "metadata": {
        "id": "27c4abe5-1080-4046-b878-d12c67317ee6"
      },
      "outputs": [],
      "source": [
        "cpp_features = ['cpp_amean', 'cpp_stddevNorm', 'cpp_percentile20', 'cpp_percentile80']"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}